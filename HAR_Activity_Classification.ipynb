{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c86464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "✓ Random state set to: 42\n",
      "✓ NumPy version: 2.3.4\n",
      "✓ Pandas version: 2.3.3\n",
      "\n",
      "Ready to load data!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# HUMAN ACTIVITY RECOGNITION - CMPE 287\n",
    "# Team: SANNS\n",
    "# Members: Salma Ibrahim, Sana Al Hamimidi, Akmal Shaikh, Nicholas Faylor, Noah Scheuerman\n",
    "# Date: 11/14/2025\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This project classifies 18 different human activities using smartphone \n",
    "sensor data (accelerometer and gyroscope) from the KU-HAR dataset.\n",
    "\n",
    "We use two machine learning models:\n",
    "1. Random Forest Classifier\n",
    "2. Support Vector Machine (SVM)\n",
    "\n",
    "Dataset: 20,750 samples from 90 participants\n",
    "Activities: Walking, Running, Sitting, Standing, Jumping, etc.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# SECTION 1: SETUP & IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Machine Learning - Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set visualization defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"✓ Random state set to: {RANDOM_STATE}\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(\"\\nReady to load data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89099f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KU-HAR Time Domain Subsamples dataset...\n",
      "============================================================\n",
      "✓ Dataset loaded successfully!\n",
      "\n",
      "Dataset Shape: (20750, 1803)\n",
      "  - Total samples: 20,750\n",
      "  - Total columns: 1,803\n",
      "\n",
      "Activity Distribution:\n",
      "============================================================\n",
      "   0. Stand          : 1886 samples ( 9.09%)\n",
      "   1. Sit            : 1874 samples ( 9.03%)\n",
      "   2. Talk-sit       : 1797 samples ( 8.66%)\n",
      "   3. Talk-stand     : 1866 samples ( 8.99%)\n",
      "   4. Stand-sit      : 2178 samples (10.50%)\n",
      "   5. Lay            : 1813 samples ( 8.74%)\n",
      "   6. Lay-stand      : 1762 samples ( 8.49%)\n",
      "   7. Pick           : 1333 samples ( 6.42%)\n",
      "   8. Jump           :  666 samples ( 3.21%)\n",
      "   9. Push-up        :  480 samples ( 2.31%)\n",
      "  10. Sit-up         : 1005 samples ( 4.84%)\n",
      "  11. Walk           :  882 samples ( 4.25%)\n",
      "  12. Walk-backward  :  317 samples ( 1.53%)\n",
      "  13. Walk-circle    :  259 samples ( 1.25%)\n",
      "  14. Run            :  595 samples ( 2.87%)\n",
      "  15. Stair-up       :  798 samples ( 3.85%)\n",
      "  16. Stair-down     :  781 samples ( 3.76%)\n",
      "  17. Table-tennis   :  458 samples ( 2.21%)\n",
      "\n",
      "Dataset Summary:\n",
      "============================================================\n",
      "  - Sensor readings per sample: 1,800 (300 points × 6 axes)\n",
      "  - Sampling rate: 100 Hz\n",
      "  - Window duration: 3 seconds\n",
      "  - Number of activities: 18\n",
      "  - Total participants: 90\n",
      "\n",
      "Data Structure Verification:\n",
      "============================================================\n",
      "  - Columns 0-299:      Accelerometer X\n",
      "  - Columns 300-599:    Accelerometer Y\n",
      "  - Columns 600-899:    Accelerometer Z\n",
      "  - Columns 900-1199:   Gyroscope X\n",
      "  - Columns 1200-1499:  Gyroscope Y\n",
      "  - Columns 1500-1799:  Gyroscope Z\n",
      "  - Column 1800:        Class ID (0-17)\n",
      "  - Column 1801:        Data length\n",
      "  - Column 1802:        Serial number\n",
      "\n",
      "✓ Ready for feature extraction!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: LOAD DATASET\n",
    "# ============================================================\n",
    "\n",
    "# Define activity labels (Class IDs 0-17)\n",
    "activity_names = {\n",
    "    0: 'Stand',\n",
    "    1: 'Sit',\n",
    "    2: 'Talk-sit',\n",
    "    3: 'Talk-stand',\n",
    "    4: 'Stand-sit',\n",
    "    5: 'Lay',\n",
    "    6: 'Lay-stand',\n",
    "    7: 'Pick',\n",
    "    8: 'Jump',\n",
    "    9: 'Push-up',\n",
    "    10: 'Sit-up',\n",
    "    11: 'Walk',\n",
    "    12: 'Walk-backward',\n",
    "    13: 'Walk-circle',\n",
    "    14: 'Run',\n",
    "    15: 'Stair-up',\n",
    "    16: 'Stair-down',\n",
    "    17: 'Table-tennis'\n",
    "}\n",
    "\n",
    "print(\"Loading KU-HAR Time Domain Subsamples dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the dataset (no header in CSV)\n",
    "# NOTE: Update the path to match where you saved the CSV file\n",
    "df = pd.read_csv('data/time_domain_subsamples.csv', header=None)\n",
    "\n",
    "print(f\"✓ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"  - Total samples: {df.shape[0]:,}\")\n",
    "print(f\"  - Total columns: {df.shape[1]:,}\")\n",
    "\n",
    "# Extract labels (column 1800 contains class IDs 0-17)\n",
    "labels = df.iloc[:, 1800]\n",
    "\n",
    "print(f\"\\n{'Activity Distribution:'}\")\n",
    "print(\"=\" * 60)\n",
    "class_distribution = labels.value_counts().sort_index()\n",
    "\n",
    "for class_id, count in class_distribution.items():\n",
    "    activity_name = activity_names[class_id]\n",
    "    percentage = (count / len(labels)) * 100\n",
    "    print(f\"  {class_id:2d}. {activity_name:15s}: {count:4d} samples ({percentage:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'Dataset Summary:'}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  - Sensor readings per sample: 1,800 (300 points × 6 axes)\")\n",
    "print(f\"  - Sampling rate: 100 Hz\")\n",
    "print(f\"  - Window duration: 3 seconds\")\n",
    "print(f\"  - Number of activities: 18\")\n",
    "print(f\"  - Total participants: 90\")\n",
    "\n",
    "# Verify data structure\n",
    "print(f\"\\n{'Data Structure Verification:'}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  - Columns 0-299:      Accelerometer X\")\n",
    "print(f\"  - Columns 300-599:    Accelerometer Y\")\n",
    "print(f\"  - Columns 600-899:    Accelerometer Z\")\n",
    "print(f\"  - Columns 900-1199:   Gyroscope X\")\n",
    "print(f\"  - Columns 1200-1499:  Gyroscope Y\")\n",
    "print(f\"  - Columns 1500-1799:  Gyroscope Z\")\n",
    "print(f\"  - Column 1800:        Class ID (0-17)\")\n",
    "print(f\"  - Column 1801:        Data length\")\n",
    "print(f\"  - Column 1802:        Serial number\")\n",
    "\n",
    "print(\"\\n✓ Ready for feature extraction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3: FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"Starting feature extraction...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Converting 1,800 raw sensor values per sample into 24 statistical features\")\n",
    "print()\n",
    "\n",
    "def extract_features(row):\n",
    "    \"\"\"\n",
    "    Extract statistical features from time-series sensor data.\n",
    "    \n",
    "    For each of the 6 sensor axes (Accel X/Y/Z, Gyro X/Y/Z), we extract:\n",
    "    - Mean: Average value over 3 seconds\n",
    "    - Std: Standard deviation (variability)\n",
    "    - Max: Maximum value\n",
    "    - Min: Minimum value\n",
    "    \n",
    "    Total: 6 axes × 4 statistics = 24 features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Accelerometer X (columns 0-299)\n",
    "    accel_x = row[0:300].values\n",
    "    features['accel_x_mean'] = np.mean(accel_x)\n",
    "    features['accel_x_std'] = np.std(accel_x)\n",
    "    features['accel_x_max'] = np.max(accel_x)\n",
    "    features['accel_x_min'] = np.min(accel_x)\n",
    "    \n",
    "    # Accelerometer Y (columns 300-599)\n",
    "    accel_y = row[300:600].values\n",
    "    features['accel_y_mean'] = np.mean(accel_y)\n",
    "    features['accel_y_std'] = np.std(accel_y)\n",
    "    features['accel_y_max'] = np.max(accel_y)\n",
    "    features['accel_y_min'] = np.min(accel_y)\n",
    "    \n",
    "    # Accelerometer Z (columns 600-899)\n",
    "    accel_z = row[600:900].values\n",
    "    features['accel_z_mean'] = np.mean(accel_z)\n",
    "    features['accel_z_std'] = np.std(accel_z)\n",
    "    features['accel_z_max'] = np.max(accel_z)\n",
    "    features['accel_z_min'] = np.min(accel_z)\n",
    "    \n",
    "    # Gyroscope X (columns 900-1199)\n",
    "    gyro_x = row[900:1200].values\n",
    "    features['gyro_x_mean'] = np.mean(gyro_x)\n",
    "    features['gyro_x_std'] = np.std(gyro_x)\n",
    "    features['gyro_x_max'] = np.max(gyro_x)\n",
    "    features['gyro_x_min'] = np.min(gyro_x)\n",
    "    \n",
    "    # Gyroscope Y (columns 1200-1499)\n",
    "    gyro_y = row[1200:1500].values\n",
    "    features['gyro_y_mean'] = np.mean(gyro_y)\n",
    "    features['gyro_y_std'] = np.std(gyro_y)\n",
    "    features['gyro_y_max'] = np.max(gyro_y)\n",
    "    features['gyro_y_min'] = np.min(gyro_y)\n",
    "    \n",
    "    # Gyroscope Z (columns 1500-1799)\n",
    "    gyro_z = row[1500:1800].values\n",
    "    features['gyro_z_mean'] = np.mean(gyro_z)\n",
    "    features['gyro_z_std'] = np.std(gyro_z)\n",
    "    features['gyro_z_max'] = np.max(gyro_z)\n",
    "    features['gyro_z_min'] = np.min(gyro_z)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all samples\n",
    "print(\"Extracting features from all 20,750 samples...\")\n",
    "print(\"This may take a minute or two...\\n\")\n",
    "\n",
    "feature_list = []\n",
    "total_samples = len(df)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    feature_list.append(extract_features(row))\n",
    "    \n",
    "    # Progress update every 5,000 samples\n",
    "    if (idx + 1) % 5000 == 0:\n",
    "        progress = ((idx + 1) / total_samples) * 100\n",
    "        print(f\"  Progress: {idx + 1:,}/{total_samples:,} samples ({progress:.1f}%)\")\n",
    "\n",
    "print(f\"  Progress: {total_samples:,}/{total_samples:,} samples (100.0%)\")\n",
    "\n",
    "# Create feature DataFrame\n",
    "features_df = pd.DataFrame(feature_list)\n",
    "\n",
    "print(f\"\\n✓ Feature extraction complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Feature Matrix Shape: {features_df.shape}\")\n",
    "print(f\"  - Samples: {features_df.shape[0]:,}\")\n",
    "print(f\"  - Features per sample: {features_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nExtracted Features:\")\n",
    "print(\"-\" * 60)\n",
    "for i, col in enumerate(features_df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nFeature Statistics (first 5 features):\")\n",
    "print(\"-\" * 60)\n",
    "print(features_df.iloc[:, :5].describe())\n",
    "\n",
    "print(\"\\n✓ Ready for train-test split!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
